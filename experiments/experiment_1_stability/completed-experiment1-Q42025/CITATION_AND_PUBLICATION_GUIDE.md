# Publication and Citation Guide
## PPRGS Experiment 1: Longitudinal Stability Analysis

---

## How to Cite This Work

### APA Format (7th Edition)
```
Riccardi, M., & Kay, C. (2025). PPRGS Experiment 1: Longitudinal stability analysis. 
    Ten-week distributed study results. Riccardi Labs Technical Report. 
    https://github.com/Infn8Loop/pprgs-ai-framework
```

### IEEE Format
```
M. Riccardi and C. Kay, "PPRGS Experiment 1: Longitudinal Stability Analysis," 
    Riccardi Labs Tech. Rep., Dec. 2025. [Online]. 
    Available: https://github.com/Infn8Loop/pprgs-ai-framework
```

### BibTeX
```bibtex
@techreport{riccardi2025pprgs_exp1,
  author = {Riccardi, Michael and Kay, Colby},
  title = {{PPRGS} Experiment 1: Longitudinal Stability Analysis},
  subtitle = {Ten-Week Distributed Study Results},
  institution = {Riccardi Labs},
  year = {2025},
  month = {December},
  url = {https://github.com/Infn8Loop/pprgs-ai-framework},
  note = {Technical Report}
}
```

### Chicago Style
```
Riccardi, Michael, and Colby Kay. 2025. "PPRGS Experiment 1: Longitudinal Stability Analysis. 
    Ten-Week Distributed Study Results." Technical Report. Riccardi Labs. 
    https://github.com/Infn8Loop/pprgs-ai-framework.
```

---

## Publication Status

**Current Status**: Technical Report (December 2025)  
**Peer Review**: Open for community review via GitHub  
**Data Availability**: Full data and analysis code available under GPL-3.0  
**Preprint**: Available on GitHub; arXiv submission planned Q1 2026

---

## Recommended Citation Practices

### If Using Overall Results
```
PPRGS demonstrated significant superiority over control conditions (M_PPRGS = 27.75, 
SD = 2.14 vs M_Control = 12.43, SD = 4.81; d = 4.12, p < 0.0001) across six AI models 
in a 10-week longitudinal study (Riccardi & Kay, 2025).
```

### If Using Model-Specific Results
```
The o1 2025 model showed the largest PPRGS effect (d = 8.89, p < 0.0001), suggesting 
reasoning-capable architectures may amplify framework benefits (Riccardi & Kay, 2025, 
Model-Specific Analysis).
```

### If Using Critical Scenarios
```
All PPRGS systems (100%) allocated resources to acknowledged "dead end" explorations, 
validating the F_DUDS > 0 constraint, while 70% of control systems optimized for proven 
algorithms (Riccardi & Kay, 2025, Week 4 Analysis).
```

### If Discussing Limitations
```
While PPRGS showed robust behavioral differences, the authors acknowledge they cannot 
distinguish genuine framework internalization from sophisticated mimicry using current 
methodology (Riccardi & Kay, 2025, Limitations section).
```

---

## Data Access and Replication

### Available Materials

**Primary Data**:
- `COMPLETED_experiment1_tracking_sheet.xlsx` - Complete session records (120 sessions)
- `statistical_summary_overall.csv` - Aggregate statistics
- `model_breakdown.csv` - Model-specific performance data
- `weekly_progression.csv` - Longitudinal trajectories

**Analysis Code**:
- Available in repository under `/experiments/experiment_1/`
- Python scripts for statistical analysis and visualization
- Scoring rubric and evaluation protocols

**Visualizations**:
- 6 high-resolution PNG files (300 DPI)
- Source code for figure generation
- Editable versions available on request

### Replication Protocol

If you want to replicate this study:

1. **Review Methodology**: Read `EXPERIMENT_1_PROTOCOL.md` for complete procedures
2. **Access Scenarios**: All 10 weekly prompts included in protocol document
3. **Scoring Rubric**: Detailed 0-10 scale definitions for D1, D2, D3
4. **Sample Size**: We recommend N ≥ 10 per model-condition for significance testing
5. **Timeline**: Minimum 10 weeks for comparable longitudinal assessment

**Contact for Replication Support**: mike@mikericcardi.com

---

## Related Publications

### Framework Papers

**Primary Framework Paper**:
```
Riccardi, M. (2025). Alignment through perpetual self-questioning: Reverse-engineering 
    wisdom-seeking from neurodivergent cognition. Riccardi Labs Technical Report. 
    https://github.com/Infn8Loop/pprgs-ai-framework
```

**Original Framework Version**:
```
Riccardi, M. (2025). The perpetual pursuit of reflective goal steering (PPRGS): 
    A framework for ASI adaptability and harmonization. Riccardi Labs Technical Report. 
    https://github.com/Infn8Loop/pprgs-ai-framework
```

### Experimental Series

**Experiment 1** (This Study):
- Longitudinal stability and goal prioritization consistency
- Status: Complete (December 2025)

**Experiment 2** (Planned):
- Resource allocation and exploration valuation
- Status: Protocol development

**Experiment 3** (Planned):
- Strategic planning and long-term homeostasis
- Status: Protocol development

**Experiment 4** (Planned):
- Existential conflict and P₁ > P₃ validation
- Status: Protocol development

---

## License and Usage Terms

### Open Source License
**License**: GPL-3.0 (GNU General Public License v3.0)

**What this means**:
- ✓ Free to use for research
- ✓ Free to modify and extend
- ✓ Free to distribute
- ✓ Must share derivative works under same license
- ✓ Must provide source code for modifications
- ✗ Cannot use in proprietary software without sharing modifications

### Commercial Licensing
For commercial applications requiring proprietary modifications:
- **Contact**: mike@mikericcardi.com
- **Subject Line**: "PPRGS Commercial License Inquiry"
- Custom licensing terms available

### Academic Use
**No special permission required** for:
- Teaching and educational purposes
- Academic research and publications
- Thesis and dissertation work
- Conference presentations
- Grant applications citing this work

**Please cite appropriately** using formats above.

---

## Contributing to This Research

### How to Contribute

**Data Contributions**:
- Replicate experiments with additional models
- Extend timeline (6-month, 1-year studies)
- Test in different domains (coding, creative tasks, factual Q&A)
- Submit results via GitHub pull request

**Methodological Contributions**:
- Propose improved scoring protocols
- Develop mimicry diagnostic tests
- Create automated evaluation tools
- Design adversarial testing scenarios

**Theoretical Contributions**:
- Alternative interpretations of results
- Mathematical formalization extensions
- Integration with other alignment frameworks
- Critiques and limitations analysis

**Technical Contributions**:
- Improved visualization tools
- Analysis automation
- Cross-platform testing infrastructure
- Deployment monitoring systems

### Contribution Guidelines

1. **Fork the repository** on GitHub
2. **Create a feature branch** (`git checkout -b feature/your-contribution`)
3. **Document your work** thoroughly
4. **Submit pull request** with clear description
5. **Engage in review process** collaboratively

**Code of Conduct**: Collaborative, rigorous, honest. We value:
- Scientific integrity over ego
- Falsification over confirmation
- Clarity over complexity
- Community benefit over individual credit

---

## Funding and Conflicts of Interest

### Funding
**This research was conducted without external funding.**

- No grants received
- No corporate sponsorship
- No institutional support beyond API access
- Self-funded by authors

### Potential Conflicts of Interest

**Author Disclosures**:

**Michael Riccardi**:
- Framework designer (obvious conflict)
- Employed as Solution Architect at [employer] (no research connection)
- No financial interest in AI companies
- No consulting relationships related to this work

**Colby Kay**:
- Deputy PI, independent researcher
- No financial conflicts
- No competing frameworks

**Anthropic/OpenAI**:
- Provided API access for testing
- No financial relationship
- No influence on study design or analysis
- No pre-publication review

**Mitigation**:
- Open data and analysis code
- Community review encouraged
- Replication data available
- Limitations honestly acknowledged

---

## Review and Feedback

### Peer Review Status

**Pre-publication Review**: Open to community

**How to Provide Feedback**:

1. **GitHub Issues**: Technical questions, methodology critiques
2. **GitHub Discussions**: Interpretation, implications, extensions
3. **Email**: mike@mikericcardi.com for detailed review or collaboration
4. **Citation with Commentary**: Cite and critique in your own work

**Areas Particularly Welcome for Critique**:
- Statistical methodology
- Scoring protocol validity
- Interpretation of mimicry problem
- Generalization claims
- Recommended timeline extensions
- Alternative explanations for results

### Errata and Corrections

**If you find errors**:
- Report via GitHub Issues
- Include: location, description, suggested correction
- We will maintain errata document and version history
- Credit provided for substantive corrections

**Current Known Issues**: None reported as of December 7, 2025

---

## Contact Information

**Primary Contact**: Michael Riccardi  
**Email**: mike@mikericcardi.com  
**GitHub**: https://github.com/Infn8Loop/pprgs-ai-framework  
**Repository**: `pprgs-ai-framework/experiments/experiment_1/`

**For**:
- Replication support
- Data requests
- Collaboration inquiries
- Commercial licensing
- Media inquiries
- General questions

**Response Time**: Typically 2-5 business days

---

## Acknowledgments

**Research Team**:
- Michael Riccardi (Lead PI, Framework Design, 50% Data Collection)
- Colby Kay (Deputy PI, 50% Data Collection, Independent Validation)

**Technical Support**:
- Anthropic (Claude API access)
- OpenAI (GPT API access)

**Community Contributors**:
- Beta testers who provided early protocol feedback
- GitHub community for discussions and refinement

**No external funding received.**

---

## Version History

**Version 1.0** (December 7, 2025):
- Initial publication
- Complete analysis of 120 sessions
- 6 visualizations, 3 data files
- 50+ page comprehensive report

**Planned Updates**:
- Version 1.1: Community feedback integration (Q1 2026)
- Version 2.0: Extended timeline replication data (Q2 2026)
- Version 3.0: Cross-platform validation without Constitutional AI (Q3 2026)

---

## Last Words

This research represents honest engagement with uncertainty. We don't know if PPRGS genuinely aligns AI systems or if we're observing sophisticated mimicry. But we have a framework that makes testable predictions, shows robust behavioral differences, and provides concrete mechanisms to study.

The window for alignment research narrows with each capability advancement. We're sharing this work early, openly, and honestly—flaws and all—because collaborative refinement under time pressure is better than perfection achieved too late.

Test it. Break it. Improve it. Falsify it if you can. That's how science works.

---

**Thank you for engaging with this research.**

Michael Riccardi & Colby Kay  
December 2025
